{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rt4c6QLyCDC"
   },
   "source": [
    "## Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4ZxYZNY2yAJV"
   },
   "outputs": [],
   "source": [
    "# Base\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Graph\n",
    "import networkx as nx\n",
    "# node embedding\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphStructure():   \n",
    "    def __init__(self, G):\n",
    "          self.G = G\n",
    "\n",
    "    '''calucate disconnected pairs for negative sample'''\n",
    "    def disconnected_node_pairs(self, node_list):\n",
    "        possible_node_pairs = list()\n",
    "        adjacency_matrix = nx.to_numpy_array(self.G, nodelist=node_list)\n",
    "        for i in range(adjacency_matrix.shape[0]):\n",
    "            for j in range(adjacency_matrix.shape[1]):\n",
    "                if i != j:\n",
    "                    try:\n",
    "                        n = nx.shortest_path_length(G, str(i), str(j))\n",
    "                    except:\n",
    "                        n = 0\n",
    "                    if n <= 2 and adjacency_matrix[i, j] == 0:\n",
    "                        possible_node_pairs.append((node_list[i], node_list[j]))\n",
    "#                 if i != j and adjacency_matrix[i][j] == 0:\n",
    "#                     possible_node_pairs.append((node_list[i], node_list[j]))\n",
    "        return possible_node_pairs\n",
    "\n",
    "    '''calucate removable pairs for positive sample'''\n",
    "    def removable_node_pairs(self, node_pairs_df):\n",
    "        # check whether removing a node pair will cause\n",
    "        # 1: graphic segmentation\n",
    "        # 2: reduce the number of nodes\n",
    "        removable_links_index = list()\n",
    "        original_node_num = self.G.number_of_nodes()\n",
    "        temp_node_pairs_df = node_pairs_df.copy()\n",
    "        for i in tqdm(node_pairs_df.index.values):\n",
    "            temp_G = nx.from_pandas_edgelist(temp_node_pairs_df.drop(index = i), \"node1\", \"node2\", create_using=nx.Graph())\n",
    "            if (nx.number_connected_components(temp_G) == 1) and (temp_G.number_of_nodes() == original_node_num):\n",
    "                removable_links_index.append(i)\n",
    "                temp_node_pairs_df = temp_node_pairs_df.drop(index = i) \n",
    "        return removable_links_index\n",
    "\n",
    "def load_dataset(file_path, split_symbol, read_title=False):\n",
    "    node_pairs = list()\n",
    "    with open(file_path, 'r') as f:\n",
    "        if read_title:\n",
    "            title = f.readline()\n",
    "        for line in f.readlines():\n",
    "            node_pairs.append(list(line.strip().split(split_symbol)))\n",
    "        dataset_df = pd.DataFrame(node_pairs, columns=['node1', 'node2'])\n",
    "    return dataset_df\n",
    "\n",
    "def preprocess(node_pairs_df):\n",
    "    instances = list()\n",
    "    for i, row in node_pairs_df.iterrows():\n",
    "        s_index, t_index, label = row\n",
    "        instance = {\n",
    "            'source': torch.LongTensor(np.array([int(s_index)-1])),\n",
    "            'target': torch.LongTensor(np.array([int(t_index)-1])),\n",
    "            'label': torch.FloatTensor(np.array([float(label)]))\n",
    "        }\n",
    "        instances.append(instance)\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzsk_iJRnJMK"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5RAaK_30bWj",
    "outputId": "a6da3d6f-a2bc-4dec-c36e-5b3e86e395da"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Random seed\n",
    "    seed = 42\n",
    "    valid_sample_ratio = 0.1\n",
    "    test_sample_ratio = 0.2\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "#     node_pairs_df = load_dataset('fb-pages-food.edges', split_symbol=',', read_title=False)\n",
    "    node_pairs_df = load_dataset('out.dimacs10-polblogs', split_symbol='\\t', read_title=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnO26buXBQaQ"
   },
   "source": [
    "## Dataset Splitting and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total # of nodes: 1224\n",
      "total # of edges: 16715\n"
     ]
    }
   ],
   "source": [
    "    # node_pairs = [ pair for pair in zip(node_pairs_df['node1'], node_pairs_df['node2'])]\n",
    "    test_snapshot = nx.from_pandas_edgelist(node_pairs_df, 'node1', 'node2', create_using=nx.Graph())\n",
    "    test_node_pairs_df = pd.DataFrame(list(test_snapshot.edges()), columns=['node1', 'node2'])    \n",
    "    print('total # of nodes:', test_snapshot.number_of_nodes())\n",
    "    print('total # of edges:', test_snapshot.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test # of negative: 1463522\t# of positive: 3343\n",
      "sample after:\n",
      "# of negative: 3343\t# of positive: 3343\n",
      "\n",
      "        node1 node2  label\n",
      "666016    410   281      0\n",
      "1089926  1166   248      0\n",
      "1247237   632   732      0\n",
      "364504    118   612      0\n",
      "566664    186   632      0\n",
      "...       ...   ...    ...\n",
      "3338      234   644      1\n",
      "3339      231   397      1\n",
      "3340      312   583      1\n",
      "3341      177   181      1\n",
      "3342      129   292      1\n",
      "\n",
      "[6686 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "    test_gs = GraphStructure(test_snapshot)\n",
    "    # sampling edges(negative)\n",
    "    test_no_edge_pairs = test_gs.disconnected_node_pairs(list(dict.fromkeys(node_pairs_df['node1'].to_list()+node_pairs_df['node2'].to_list())))\n",
    "    test_no_edge_pairs_df = pd.DataFrame(test_no_edge_pairs, columns=['node1', 'node2'])\n",
    "    test_negative_df = test_no_edge_pairs_df\n",
    "    \n",
    "    # to run for about 6 mins \n",
    "    # removable_node_pairs_index = gs.removable_node_pairs(node_pairs_df)\n",
    "    \n",
    "    # sampling edges(postive)\n",
    "    sample_ratio = 0.2\n",
    "    test_positive_instance = random.sample(list(test_snapshot.edges()), int(test_snapshot.number_of_edges()*sample_ratio))\n",
    "    test_positive_df = pd.DataFrame(test_positive_instance, columns=['node1', 'node2'])\n",
    "    \n",
    "    # labeling\n",
    "    test_negative_df['label'] = 0\n",
    "    test_positive_df['label'] = 1\n",
    "    print(\"test # of negative: %d\\t# of positive: %d\" % (len(test_negative_df), len(test_positive_df)))\n",
    "    \n",
    "    test_negative_df = test_negative_df.sample(len(test_positive_df), replace=True)\n",
    "    test_dataset_df = test_negative_df.append(test_positive_df)\n",
    "    test_positive_num, test_negative_num = test_dataset_df.label.value_counts()\n",
    "    print(\"sample after:\\n# of negative: %d\\t# of positive: %d\\n\" % (test_positive_num, test_negative_num))\n",
    "    print(test_dataset_df)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of negative: 1395312\t# of positive: 13372\n",
      "sample after:\n",
      "# of negative: 13372\t# of positive: 13372\n",
      "\n",
      "        node1 node2  label\n",
      "1131863   511   764      0\n",
      "296741    277   146      0\n",
      "1314033  1106  1108      0\n",
      "1107438   964  1063      0\n",
      "54017      50   190      0\n",
      "...       ...   ...    ...\n",
      "13367    1081  1155      1\n",
      "13368    1117  1157      1\n",
      "13369    1168  1210      1\n",
      "13370    1180  1181      1\n",
      "13371    1189  1213      1\n",
      "\n",
      "[26744 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "    previous_snapshot = test_snapshot.copy()\n",
    "    # remove postive edges\n",
    "    for pair in test_positive_instance:\n",
    "        previous_snapshot.remove_edge(*pair)\n",
    "        \n",
    "    train_positive_df = pd.DataFrame(previous_snapshot.edges(), columns=['node1', 'node2'])\n",
    "    train_gs = GraphStructure(previous_snapshot)\n",
    "    train_no_edge_pairs = train_gs.disconnected_node_pairs(list(dict.fromkeys(train_positive_df['node1'].to_list()+train_positive_df['node2'].to_list())))\n",
    "    train_no_edge_pairs_df = pd.DataFrame(train_no_edge_pairs, columns=['node1', 'node2'])\n",
    "    train_negative_df = train_no_edge_pairs_df\n",
    "    \n",
    "    # labeling\n",
    "    train_negative_df['label'] = 0\n",
    "    train_positive_df['label'] = 1\n",
    "    print(\"# of negative: %d\\t# of positive: %d\" % (len(train_negative_df), len(train_positive_df)))\n",
    "\n",
    "    train_negative_df = train_negative_df.sample(len(train_positive_df), replace=True)\n",
    "    train_dataset_df = train_negative_df.append(train_positive_df)\n",
    "    train_positive_num, train_negative_num = train_dataset_df.label.value_counts()\n",
    "    print(\"sample after:\\n# of negative: %d\\t# of positive: %d\\n\" % (train_positive_num, train_negative_num))\n",
    "    print(train_dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train instances: 26744\n",
      "# of test instances: 6686\n",
      "# of total instances: 33430\n"
     ]
    }
   ],
   "source": [
    "    test_instances = preprocess(test_dataset_df)\n",
    "    train_instances = preprocess(train_dataset_df)\n",
    "    \n",
    "    print('# of train instances:', len(train_instances))\n",
    "    print('# of test instances:', len(test_instances))\n",
    "    print('# of total instances:', len(train_instances)+len(test_instances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rTSUoWMOmeZ"
   },
   "source": [
    "## Graph Node Embedding with Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "11119ea4b95b49ccb656a7a65e90673b",
      "bcb19d65b8c4409392ad357b90149e5e",
      "781c3f8aae4844b6942e9198e9706f5b",
      "1b2087ddaab14acab394bb565434203e",
      "a9189f6de19343ec9904ae92b067e136",
      "9833efb2f49448768d5797689ff17cd2",
      "1f5c969dbd8f41719a5850f8bc7c7406",
      "1efa363c64184e988cd92b9ff0b879f1"
     ]
    },
    "id": "K-91l9ElOnDw",
    "outputId": "d23bcbbb-6976-413e-d92c-8b89c934be75",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e014373f7788490fb897e9ff29c2b4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Computing transition probabilities'), FloatProgress(value=0.0, max=1224.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1):   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [00:36<00:00,  3.69s/it]\n"
     ]
    }
   ],
   "source": [
    "    node2vec = Node2Vec(previous_snapshot, dimensions=128, walk_length=80, num_walks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqsoVI4PtZXY",
    "outputId": "3011aa12-dc21-4a1c-a81e-b0b0da70ec54"
   },
   "outputs": [],
   "source": [
    "    n2v_model = node2vec.fit(window=10, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1224, 128)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    node_embedding = n2v_model.wv.vectors\n",
    "    node_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class NodePairDataset(Dataset):\n",
    "        def __init__(self, instances):\n",
    "            self.instances = instances\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.instances)\n",
    "\n",
    "        def __getitem__(self, i):\n",
    "            instance = self.instances[i]\n",
    "            source = instance['source']\n",
    "            target = instance['target']\n",
    "            label = instance['label']\n",
    "            return source, target, label\n",
    "        \n",
    "    def collate_fn(batch):\n",
    "        source, target, labels = zip(*batch)\n",
    "        source = torch.stack(source)\n",
    "        target = torch.stack(target)\n",
    "        labels = torch.stack(labels)\n",
    "        return source, target, labels\n",
    "\n",
    "    def get_dataloader(instances, collate_fn=collate_fn,batch_size=1, num_workers=2):\n",
    "        dataset = NodePairDataset(instances)\n",
    "        dataloader = DataLoader(dataset, collate_fn=collate_fn, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class LinkEmbedding(nn.Module):\n",
    "        def __init__(self, inputs_dim, output_dim):\n",
    "            super(LinkEmbedding, self).__init__()\n",
    "            self.weight = nn.Parameter(nn.init.xavier_uniform_(torch.empty(inputs_dim, output_dim)))\n",
    "            \n",
    "            \n",
    "        def forward(self, hidden_state, source, target):\n",
    "            propagation = torch.mul(hidden_state[source, :], hidden_state[target, :])\n",
    "            propagation = propagation.matmul(self.weight)\n",
    "            return propagation\n",
    "    \n",
    "    class GraphConvolution(nn.Module):\n",
    "        def __init__(self, inputs_dim, hidden_features):\n",
    "            super(GraphConvolution, self).__init__()\n",
    "            self.weight = nn.Parameter(nn.init.kaiming_normal_(torch.empty(inputs_dim, hidden_features), mode='fan_in', nonlinearity='relu'))\n",
    "            \n",
    "        def forward(self, input_features, adj_matrix):\n",
    "            # aggregate \n",
    "            aggregate  = torch.mm(input_features, self.weight)\n",
    "            propagation = torch.mm(adj_matrix, aggregate)\n",
    "            return propagation\n",
    "        \n",
    "    class GCN(nn.Module):\n",
    "        def __init__(self, inputs_dim, hidden_dim, output_dim, class_num=1):\n",
    "            super(GCN, self).__init__()\n",
    "            self.gcn_layer1 = GraphConvolution(inputs_dim, hidden_dim)\n",
    "            self.gcn_layer2 = GraphConvolution(hidden_dim, hidden_dim)\n",
    "            self.link_embed_layer = LinkEmbedding(hidden_dim, class_num)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, input_features, adj_matrix, source, target):\n",
    "            hidden_state = self.relu(self.gcn_layer1(input_features, adj_matrix))\n",
    "            hidden_state = self.gcn_layer2(hidden_state, adj_matrix)\n",
    "            hidden_state = self.link_embed_layer(hidden_state, source, target)\n",
    "            return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class GCNTrainer():\n",
    "        def __init__(self, features, adj_matrix, train_instances, valid_instances=None, test_instances=None, \n",
    "            hidden_dim=16, epoch=1, learning_rate=1e-2, batch_size=1,num_workers=2, valid=False):\n",
    "\n",
    "            # parameters\n",
    "            self.valid = valid\n",
    "            self.epochs = epoch\n",
    "            self.learning_rate = learning_rate\n",
    "            self.batch_size = batch_size\n",
    "            self.num_workers = num_workers\n",
    "            # early stop\n",
    "            self.best_valid_loss = 1e10\n",
    "            self.max_patience = 0\n",
    "            self.patience = 0\n",
    "\n",
    "            # setup cuda device\n",
    "            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            # dataset\n",
    "            self.train_instances = train_instances\n",
    "            self.valid_instances = valid_instances\n",
    "            self.test_instances = test_instances\n",
    "            self.features = torch.FloatTensor(features).cuda()\n",
    "            self.adj_matrix = torch.FloatTensor(self.normalize(adj_matrix)).cuda()\n",
    "            \n",
    "            # GCN Model\n",
    "            self.model = GCN(self.features.shape[1], hidden_dim, 1)\n",
    "            self.model.cuda()\n",
    "            # print(self.model)\n",
    "\n",
    "            # AdamW optimizer with hyper-parameter\n",
    "            self.optimizer = Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "            # Binary Cross Entropy with Loss for criterion\n",
    "            self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "\n",
    "        def normalize(self, A):\n",
    "            '''\n",
    "            :var I: identity matrix\n",
    "            :var A: adjacency matrix\n",
    "            :var D: degree matrix\n",
    "            :var A_hat: adding self-loops\n",
    "            :var D_inv: degree inverse matrix\n",
    "            '''\n",
    "            I = np.matrix(np.identity(A.shape[0]))\n",
    "            A_hat = I + A\n",
    "            \n",
    "            D = np.array(np.sum(A, axis=0))\n",
    "            D_inv = D**-0.5\n",
    "            D_inv[np.isinf(D_inv)] = 0.\n",
    "            D_inv = np.diag(D_inv)\n",
    "\n",
    "            A_hat = D_inv * A_hat * D_inv\n",
    "            return A_hat\n",
    "        \n",
    "        def accuracy(self, predicts, labels):\n",
    "            predicts_labels = torch.round(torch.sigmoid(predicts))\n",
    "            total_correct = (predicts_labels == labels).sum().float()\n",
    "            return torch.round((total_correct / labels.shape[0]) * 100)\n",
    "\n",
    "        def train(self):\n",
    "            start_time = time()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            train_dataloader = get_dataloader(self.train_instances, collate_fn=collate_fn, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "            for epoch in range(self.epochs):\n",
    "                self.model.train()\n",
    "                epoch_loss, epoch_acc = 0, 0\n",
    "                ''' train '''\n",
    "                for i, batch in enumerate(train_dataloader, start=1):\n",
    "                    batch = (tensor.cuda() for tensor in batch)\n",
    "                    source, target, labels = batch\n",
    "                    # forward\n",
    "                    # feature: all node embedding\n",
    "                    outputs = self.model(self.features, self.adj_matrix, source, target)\n",
    "                    outputs = outputs.reshape(labels.size())\n",
    "                    # backward\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    acc = self.accuracy(outputs, labels)\n",
    "                    epoch_loss += loss.item()\n",
    "                    epoch_acc += acc\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    # optimize\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    # Progressbar\n",
    "                    elapsed_time = time() - start_time\n",
    "                    elapsed_time = timedelta(seconds=int(elapsed_time))\n",
    "                    # print(\"Epoch %d/%d | loss: %.6f | acc: %f | batch: [%d/%d] | %s\" % (epoch+1, self.epochs, loss, acc, i, len(train_dataloader), elapsed_time))\n",
    "                \n",
    "                print(\"Epoch %d/%d - train_loss: %.6f - train_acc: %f\" \n",
    "                      % (epoch+1, self.epochs, epoch_loss/len(train_dataloader), epoch_acc/len(train_dataloader)))\n",
    "                \n",
    "                ''' validate '''\n",
    "                if self.valid:\n",
    "                    valid_loss, valid_acc = self.validate()\n",
    "                    elapsed_time = time() - start_time\n",
    "                    elapsed_time = timedelta(seconds=int(elapsed_time))\n",
    "                    print(\"Epoch %d/%d - valid_loss: %.6f - valid_acc: %f\" % (epoch+1, self.epochs, valid_loss, valid_acc))\n",
    "\n",
    "                    # early stoping\n",
    "                    if valid_loss < self.best_valid_loss:\n",
    "                        self.patience = 0\n",
    "                        self.best_valid_loss = valid_loss\n",
    "                    else:\n",
    "                        self.patience += 1\n",
    "\n",
    "                    if self.patience > self.max_patience:\n",
    "                        print('Earlystop at epoch %d' % (epoch+1))\n",
    "                        break\n",
    "\n",
    "\n",
    "        def validate(self):\n",
    "            total_loss, total_acc = 0, 0\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                valid_dataloader = get_dataloader(self.valid_instances, collate_fn=collate_fn, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "                for batch in valid_dataloader:\n",
    "                    batch = (tensor.cuda() for tensor in batch)\n",
    "                    source, target, labels = batch\n",
    "                    outputs = self.model(self.features, self.adj_matrix, source, target)\n",
    "                    outputs = outputs.reshape(labels.size())\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    # loss and accuracy\n",
    "                    total_loss += loss.item()\n",
    "                    total_acc += self.accuracy(outputs, labels)\n",
    "            \n",
    "            total_loss /= len(valid_dataloader)\n",
    "            total_acc /= len(valid_dataloader)\n",
    "            return float(total_loss), float(total_acc)\n",
    "\n",
    "        def test(self):\n",
    "            total_loss, total_acc = 0, 0\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_dataloader = get_dataloader(self.test_instances, collate_fn=collate_fn, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "                for batch in test_dataloader:\n",
    "                    batch = (tensor.cuda() for tensor in batch)\n",
    "                    source, target, labels = batch\n",
    "                    outputs = self.model(self.features, self.adj_matrix, source, target)\n",
    "                    outputs = outputs.reshape(labels.size())\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    # loss and accuracy\n",
    "                    total_loss += loss.item()\n",
    "                    total_acc += self.accuracy(outputs, labels)\n",
    "            \n",
    "            total_loss /= len(test_dataloader)\n",
    "            total_acc /= len(test_dataloader)\n",
    "            return total_loss, float(total_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "    adj_matrix = nx.to_numpy_array(previous_snapshot)\n",
    "    print(adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-f356e0cc0862>:50: RuntimeWarning: divide by zero encountered in power\n",
      "  D_inv = D**-0.5\n"
     ]
    }
   ],
   "source": [
    "    torch.cuda.empty_cache()\n",
    "    trainer = GCNTrainer(features=node_embedding, adj_matrix=adj_matrix, \n",
    "                         train_instances=train_instances, \n",
    "                         valid_instances=None,\n",
    "                         test_instances=test_instances,\n",
    "                         hidden_dim=64, epoch=300, learning_rate=1e-2, batch_size=128, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 - train_loss: 0.568621 - train_acc: 67.239235\n",
      "Epoch 2/300 - train_loss: 0.499618 - train_acc: 75.210526\n",
      "Epoch 3/300 - train_loss: 0.473987 - train_acc: 77.550240\n",
      "Epoch 4/300 - train_loss: 0.460492 - train_acc: 78.181816\n",
      "Epoch 5/300 - train_loss: 0.455196 - train_acc: 78.320572\n",
      "Epoch 6/300 - train_loss: 0.445189 - train_acc: 79.033493\n",
      "Epoch 7/300 - train_loss: 0.430102 - train_acc: 80.143539\n",
      "Epoch 8/300 - train_loss: 0.423889 - train_acc: 80.655502\n",
      "Epoch 9/300 - train_loss: 0.426761 - train_acc: 81.315788\n",
      "Epoch 10/300 - train_loss: 0.423019 - train_acc: 81.430618\n",
      "Epoch 11/300 - train_loss: 0.413002 - train_acc: 82.358849\n",
      "Epoch 12/300 - train_loss: 0.389020 - train_acc: 83.038277\n",
      "Epoch 13/300 - train_loss: 0.384336 - train_acc: 83.435402\n",
      "Epoch 14/300 - train_loss: 0.497431 - train_acc: 79.344498\n",
      "Epoch 15/300 - train_loss: 0.393819 - train_acc: 83.028702\n",
      "Epoch 16/300 - train_loss: 0.367515 - train_acc: 84.315788\n",
      "Epoch 17/300 - train_loss: 0.373923 - train_acc: 84.086121\n",
      "Epoch 18/300 - train_loss: 0.367558 - train_acc: 84.693779\n",
      "Epoch 19/300 - train_loss: 0.353366 - train_acc: 84.923439\n",
      "Epoch 20/300 - train_loss: 0.347216 - train_acc: 85.258369\n",
      "Epoch 21/300 - train_loss: 0.341374 - train_acc: 85.751190\n",
      "Epoch 22/300 - train_loss: 0.335229 - train_acc: 86.023918\n",
      "Epoch 23/300 - train_loss: 0.335137 - train_acc: 85.976074\n",
      "Epoch 24/300 - train_loss: 0.374528 - train_acc: 85.028702\n",
      "Epoch 25/300 - train_loss: 0.389968 - train_acc: 84.038277\n",
      "Epoch 26/300 - train_loss: 0.374234 - train_acc: 84.454544\n",
      "Epoch 27/300 - train_loss: 0.338838 - train_acc: 85.918655\n",
      "Epoch 28/300 - train_loss: 0.337267 - train_acc: 86.258369\n",
      "Epoch 29/300 - train_loss: 0.341423 - train_acc: 85.822960\n",
      "Epoch 30/300 - train_loss: 0.336836 - train_acc: 86.119614\n",
      "Epoch 31/300 - train_loss: 0.348701 - train_acc: 85.287079\n",
      "Epoch 32/300 - train_loss: 0.335280 - train_acc: 85.971291\n",
      "Epoch 33/300 - train_loss: 0.326530 - train_acc: 86.645927\n",
      "Epoch 34/300 - train_loss: 0.318858 - train_acc: 86.708130\n",
      "Epoch 35/300 - train_loss: 0.315349 - train_acc: 86.995209\n",
      "Epoch 36/300 - train_loss: 0.316629 - train_acc: 87.033493\n",
      "Epoch 37/300 - train_loss: 0.315268 - train_acc: 87.090904\n",
      "Epoch 38/300 - train_loss: 0.318675 - train_acc: 86.746407\n",
      "Epoch 39/300 - train_loss: 0.311814 - train_acc: 87.086121\n",
      "Epoch 40/300 - train_loss: 0.326066 - train_acc: 86.650711\n",
      "Epoch 41/300 - train_loss: 0.324292 - train_acc: 86.617218\n",
      "Epoch 42/300 - train_loss: 0.323677 - train_acc: 86.583733\n",
      "Epoch 43/300 - train_loss: 0.312813 - train_acc: 86.990425\n",
      "Epoch 44/300 - train_loss: 0.315191 - train_acc: 87.052628\n",
      "Epoch 45/300 - train_loss: 0.307133 - train_acc: 87.435402\n",
      "Epoch 46/300 - train_loss: 0.311737 - train_acc: 87.066986\n",
      "Epoch 47/300 - train_loss: 0.334554 - train_acc: 86.698563\n",
      "Epoch 48/300 - train_loss: 0.329003 - train_acc: 86.660286\n",
      "Epoch 49/300 - train_loss: 0.308262 - train_acc: 87.177032\n",
      "Epoch 50/300 - train_loss: 0.304888 - train_acc: 87.301430\n",
      "Epoch 51/300 - train_loss: 0.299840 - train_acc: 87.631577\n",
      "Epoch 52/300 - train_loss: 0.300192 - train_acc: 87.669853\n",
      "Epoch 53/300 - train_loss: 0.327323 - train_acc: 86.746407\n",
      "Epoch 54/300 - train_loss: 0.311948 - train_acc: 87.507172\n",
      "Epoch 55/300 - train_loss: 0.303842 - train_acc: 87.545448\n",
      "Epoch 56/300 - train_loss: 0.307502 - train_acc: 87.516747\n",
      "Epoch 57/300 - train_loss: 0.303420 - train_acc: 87.545448\n",
      "Epoch 58/300 - train_loss: 0.305097 - train_acc: 87.488037\n",
      "Epoch 59/300 - train_loss: 0.302406 - train_acc: 87.526314\n",
      "Epoch 60/300 - train_loss: 0.301559 - train_acc: 87.693779\n",
      "Epoch 61/300 - train_loss: 0.291891 - train_acc: 87.947365\n",
      "Epoch 62/300 - train_loss: 0.298020 - train_acc: 88.239235\n",
      "Epoch 63/300 - train_loss: 0.300140 - train_acc: 87.746407\n",
      "Epoch 64/300 - train_loss: 0.303244 - train_acc: 87.574158\n",
      "Epoch 65/300 - train_loss: 0.302718 - train_acc: 87.626793\n",
      "Epoch 66/300 - train_loss: 0.301198 - train_acc: 87.569374\n",
      "Epoch 67/300 - train_loss: 0.302424 - train_acc: 87.641144\n",
      "Epoch 68/300 - train_loss: 0.296256 - train_acc: 88.014351\n",
      "Epoch 69/300 - train_loss: 0.293256 - train_acc: 88.014351\n",
      "Epoch 70/300 - train_loss: 0.295951 - train_acc: 88.023918\n",
      "Epoch 71/300 - train_loss: 0.296661 - train_acc: 87.875595\n",
      "Epoch 72/300 - train_loss: 0.285437 - train_acc: 88.244019\n",
      "Epoch 73/300 - train_loss: 0.311149 - train_acc: 87.162674\n",
      "Epoch 74/300 - train_loss: 0.380041 - train_acc: 85.928230\n",
      "Epoch 75/300 - train_loss: 0.358479 - train_acc: 85.827751\n",
      "Epoch 76/300 - train_loss: 0.313033 - train_acc: 87.272720\n",
      "Epoch 77/300 - train_loss: 0.303022 - train_acc: 87.708130\n",
      "Epoch 78/300 - train_loss: 0.292703 - train_acc: 87.966507\n",
      "Epoch 79/300 - train_loss: 0.291150 - train_acc: 87.889946\n",
      "Epoch 80/300 - train_loss: 0.290044 - train_acc: 88.129181\n",
      "Epoch 81/300 - train_loss: 0.298538 - train_acc: 87.904305\n",
      "Epoch 82/300 - train_loss: 0.283386 - train_acc: 88.363632\n",
      "Epoch 83/300 - train_loss: 0.290695 - train_acc: 88.143539\n",
      "Epoch 84/300 - train_loss: 0.291283 - train_acc: 88.153107\n",
      "Epoch 85/300 - train_loss: 0.279929 - train_acc: 88.602867\n",
      "Epoch 86/300 - train_loss: 0.318222 - train_acc: 87.401909\n",
      "Epoch 87/300 - train_loss: 0.309131 - train_acc: 87.454544\n",
      "Epoch 88/300 - train_loss: 0.296234 - train_acc: 87.947365\n",
      "Epoch 89/300 - train_loss: 0.280726 - train_acc: 88.631577\n",
      "Epoch 90/300 - train_loss: 0.282120 - train_acc: 88.578941\n",
      "Epoch 91/300 - train_loss: 0.275735 - train_acc: 88.870811\n",
      "Epoch 92/300 - train_loss: 0.285525 - train_acc: 88.578941\n",
      "Epoch 93/300 - train_loss: 0.292347 - train_acc: 88.114830\n",
      "Epoch 94/300 - train_loss: 0.286653 - train_acc: 88.210526\n",
      "Epoch 95/300 - train_loss: 0.277639 - train_acc: 88.655502\n",
      "Epoch 96/300 - train_loss: 0.279042 - train_acc: 88.832535\n",
      "Epoch 97/300 - train_loss: 0.287124 - train_acc: 88.516747\n",
      "Epoch 98/300 - train_loss: 0.285708 - train_acc: 88.311005\n",
      "Epoch 99/300 - train_loss: 0.282490 - train_acc: 88.665070\n",
      "Epoch 100/300 - train_loss: 0.276059 - train_acc: 88.760765\n",
      "Epoch 101/300 - train_loss: 0.276287 - train_acc: 88.746407\n",
      "Epoch 102/300 - train_loss: 0.275909 - train_acc: 88.755974\n",
      "Epoch 103/300 - train_loss: 0.278004 - train_acc: 88.736839\n",
      "Epoch 104/300 - train_loss: 0.391136 - train_acc: 85.669853\n",
      "Epoch 105/300 - train_loss: 0.370689 - train_acc: 85.488037\n",
      "Epoch 106/300 - train_loss: 0.325083 - train_acc: 87.004784\n",
      "Epoch 107/300 - train_loss: 0.285287 - train_acc: 88.320572\n",
      "Epoch 108/300 - train_loss: 0.276891 - train_acc: 88.736839\n",
      "Epoch 109/300 - train_loss: 0.271483 - train_acc: 88.889946\n",
      "Epoch 110/300 - train_loss: 0.274068 - train_acc: 88.665070\n",
      "Epoch 111/300 - train_loss: 0.269966 - train_acc: 89.062195\n",
      "Epoch 112/300 - train_loss: 0.283842 - train_acc: 88.779900\n",
      "Epoch 113/300 - train_loss: 0.273798 - train_acc: 88.885162\n",
      "Epoch 114/300 - train_loss: 0.284220 - train_acc: 88.655502\n",
      "Epoch 115/300 - train_loss: 0.270342 - train_acc: 89.047844\n",
      "Epoch 116/300 - train_loss: 0.275723 - train_acc: 88.889946\n",
      "Epoch 117/300 - train_loss: 0.282776 - train_acc: 88.684204\n",
      "Epoch 118/300 - train_loss: 0.287662 - train_acc: 88.550240\n",
      "Epoch 119/300 - train_loss: 0.270282 - train_acc: 89.047844\n",
      "Epoch 120/300 - train_loss: 0.269654 - train_acc: 89.153107\n",
      "Epoch 121/300 - train_loss: 0.284242 - train_acc: 88.698563\n",
      "Epoch 122/300 - train_loss: 0.310734 - train_acc: 87.976074\n",
      "Epoch 123/300 - train_loss: 0.274109 - train_acc: 89.148323\n",
      "Epoch 124/300 - train_loss: 0.270610 - train_acc: 89.076553\n",
      "Epoch 125/300 - train_loss: 0.265236 - train_acc: 89.296646\n",
      "Epoch 126/300 - train_loss: 0.265881 - train_acc: 89.430618\n",
      "Epoch 127/300 - train_loss: 0.282723 - train_acc: 88.645927\n",
      "Epoch 128/300 - train_loss: 0.280155 - train_acc: 88.693779\n",
      "Epoch 129/300 - train_loss: 0.276788 - train_acc: 88.751190\n",
      "Epoch 130/300 - train_loss: 0.268032 - train_acc: 89.377991\n",
      "Epoch 131/300 - train_loss: 0.263225 - train_acc: 89.430618\n",
      "Epoch 132/300 - train_loss: 0.262527 - train_acc: 89.435402\n",
      "Epoch 133/300 - train_loss: 0.279985 - train_acc: 88.909088\n",
      "Epoch 134/300 - train_loss: 0.286619 - train_acc: 88.904305\n",
      "Epoch 135/300 - train_loss: 0.271152 - train_acc: 89.172249\n",
      "Epoch 136/300 - train_loss: 0.270936 - train_acc: 89.138756\n",
      "Epoch 137/300 - train_loss: 0.261020 - train_acc: 89.373199\n",
      "Epoch 138/300 - train_loss: 0.269451 - train_acc: 89.167458\n",
      "Epoch 139/300 - train_loss: 0.270899 - train_acc: 88.971291\n",
      "Epoch 140/300 - train_loss: 0.280587 - train_acc: 88.875595\n",
      "Epoch 141/300 - train_loss: 0.263104 - train_acc: 89.559807\n",
      "Epoch 142/300 - train_loss: 0.264953 - train_acc: 89.406693\n",
      "Epoch 143/300 - train_loss: 0.277101 - train_acc: 89.023918\n",
      "Epoch 144/300 - train_loss: 0.348265 - train_acc: 87.191383\n",
      "Epoch 145/300 - train_loss: 0.282043 - train_acc: 88.674637\n",
      "Epoch 146/300 - train_loss: 0.282333 - train_acc: 88.799042\n",
      "Epoch 147/300 - train_loss: 0.277522 - train_acc: 88.799042\n",
      "Epoch 148/300 - train_loss: 0.264407 - train_acc: 89.210526\n",
      "Epoch 149/300 - train_loss: 0.262373 - train_acc: 89.559807\n",
      "Epoch 150/300 - train_loss: 0.263619 - train_acc: 89.464111\n",
      "Epoch 151/300 - train_loss: 0.258145 - train_acc: 89.631577\n",
      "Epoch 152/300 - train_loss: 0.266987 - train_acc: 89.315788\n",
      "Epoch 153/300 - train_loss: 0.274427 - train_acc: 89.028702\n",
      "Epoch 154/300 - train_loss: 0.266255 - train_acc: 89.382774\n",
      "Epoch 155/300 - train_loss: 0.257996 - train_acc: 89.612434\n",
      "Epoch 156/300 - train_loss: 0.263465 - train_acc: 89.224876\n",
      "Epoch 157/300 - train_loss: 0.266318 - train_acc: 89.435402\n",
      "Epoch 158/300 - train_loss: 0.272011 - train_acc: 89.023918\n",
      "Epoch 159/300 - train_loss: 0.301277 - train_acc: 88.153107\n",
      "Epoch 160/300 - train_loss: 0.266912 - train_acc: 89.272720\n",
      "Epoch 161/300 - train_loss: 0.301633 - train_acc: 88.454544\n",
      "Epoch 162/300 - train_loss: 0.262579 - train_acc: 89.430618\n",
      "Epoch 163/300 - train_loss: 0.263509 - train_acc: 89.186600\n",
      "Epoch 164/300 - train_loss: 0.257456 - train_acc: 89.564590\n",
      "Epoch 165/300 - train_loss: 0.259168 - train_acc: 89.827751\n",
      "Epoch 166/300 - train_loss: 0.279962 - train_acc: 89.186600\n",
      "Epoch 167/300 - train_loss: 0.273820 - train_acc: 88.985641\n",
      "Epoch 168/300 - train_loss: 0.257659 - train_acc: 89.674637\n",
      "Epoch 169/300 - train_loss: 0.257520 - train_acc: 89.755974\n",
      "Epoch 170/300 - train_loss: 0.260049 - train_acc: 89.564590\n",
      "Epoch 171/300 - train_loss: 0.283439 - train_acc: 88.775116\n",
      "Epoch 172/300 - train_loss: 0.258954 - train_acc: 89.435402\n",
      "Epoch 173/300 - train_loss: 0.274598 - train_acc: 89.349281\n",
      "Epoch 174/300 - train_loss: 0.268530 - train_acc: 89.244019\n",
      "Epoch 175/300 - train_loss: 0.260504 - train_acc: 89.287079\n",
      "Epoch 176/300 - train_loss: 0.251647 - train_acc: 89.894737\n",
      "Epoch 177/300 - train_loss: 0.249053 - train_acc: 89.889946\n",
      "Epoch 178/300 - train_loss: 0.275187 - train_acc: 88.885162\n",
      "Epoch 179/300 - train_loss: 0.266982 - train_acc: 89.133965\n",
      "Epoch 180/300 - train_loss: 0.264716 - train_acc: 89.267937\n",
      "Epoch 181/300 - train_loss: 0.255644 - train_acc: 89.602867\n",
      "Epoch 182/300 - train_loss: 0.248876 - train_acc: 90.009567\n",
      "Epoch 183/300 - train_loss: 0.260929 - train_acc: 89.468895\n",
      "Epoch 184/300 - train_loss: 0.255999 - train_acc: 89.569374\n",
      "Epoch 185/300 - train_loss: 0.250687 - train_acc: 89.913872\n",
      "Epoch 186/300 - train_loss: 0.249543 - train_acc: 89.985641\n",
      "Epoch 187/300 - train_loss: 0.276431 - train_acc: 89.028702\n",
      "Epoch 188/300 - train_loss: 0.289613 - train_acc: 88.698563\n",
      "Epoch 189/300 - train_loss: 0.269032 - train_acc: 89.210526\n",
      "Epoch 190/300 - train_loss: 0.267275 - train_acc: 89.258369\n",
      "Epoch 191/300 - train_loss: 0.681194 - train_acc: 78.043060\n",
      "Epoch 192/300 - train_loss: 0.423779 - train_acc: 83.081337\n",
      "Epoch 193/300 - train_loss: 0.342061 - train_acc: 86.330139\n",
      "Epoch 194/300 - train_loss: 0.304468 - train_acc: 87.626793\n",
      "Epoch 195/300 - train_loss: 0.294505 - train_acc: 87.933014\n",
      "Epoch 196/300 - train_loss: 0.275874 - train_acc: 88.755974\n",
      "Epoch 197/300 - train_loss: 0.275071 - train_acc: 88.870811\n",
      "Epoch 198/300 - train_loss: 0.273296 - train_acc: 89.095688\n",
      "Epoch 199/300 - train_loss: 0.266096 - train_acc: 89.344498\n",
      "Epoch 200/300 - train_loss: 0.261868 - train_acc: 89.368416\n",
      "Epoch 201/300 - train_loss: 0.264812 - train_acc: 89.301430\n",
      "Epoch 202/300 - train_loss: 0.264052 - train_acc: 89.272720\n",
      "Epoch 203/300 - train_loss: 0.287428 - train_acc: 88.655502\n",
      "Epoch 204/300 - train_loss: 0.275744 - train_acc: 88.947365\n",
      "Epoch 205/300 - train_loss: 0.261914 - train_acc: 89.401909\n",
      "Epoch 206/300 - train_loss: 0.265568 - train_acc: 89.248802\n",
      "Epoch 207/300 - train_loss: 0.253233 - train_acc: 89.913872\n",
      "Epoch 208/300 - train_loss: 0.280142 - train_acc: 88.937798\n",
      "Epoch 209/300 - train_loss: 0.273890 - train_acc: 88.937798\n",
      "Epoch 210/300 - train_loss: 0.256323 - train_acc: 89.746407\n",
      "Epoch 211/300 - train_loss: 0.252728 - train_acc: 89.851669\n",
      "Epoch 212/300 - train_loss: 0.246603 - train_acc: 89.952148\n",
      "Epoch 213/300 - train_loss: 0.260904 - train_acc: 89.555023\n",
      "Epoch 214/300 - train_loss: 0.262702 - train_acc: 89.478462\n",
      "Epoch 215/300 - train_loss: 0.266922 - train_acc: 89.373199\n",
      "Epoch 216/300 - train_loss: 0.250215 - train_acc: 89.952148\n",
      "Epoch 217/300 - train_loss: 0.250952 - train_acc: 89.775116\n",
      "Epoch 218/300 - train_loss: 0.247577 - train_acc: 90.047844\n",
      "Epoch 219/300 - train_loss: 0.271819 - train_acc: 89.282295\n",
      "Epoch 220/300 - train_loss: 0.350992 - train_acc: 86.889946\n",
      "Epoch 221/300 - train_loss: 0.294700 - train_acc: 88.454544\n",
      "Epoch 222/300 - train_loss: 0.264728 - train_acc: 89.306213\n",
      "Epoch 223/300 - train_loss: 0.250428 - train_acc: 89.980858\n",
      "Epoch 224/300 - train_loss: 0.264049 - train_acc: 89.464111\n",
      "Epoch 225/300 - train_loss: 0.261896 - train_acc: 89.679420\n",
      "Epoch 226/300 - train_loss: 0.257954 - train_acc: 89.741623\n",
      "Epoch 227/300 - train_loss: 0.247731 - train_acc: 90.138756\n",
      "Epoch 228/300 - train_loss: 0.243740 - train_acc: 90.272720\n",
      "Epoch 229/300 - train_loss: 0.259652 - train_acc: 89.665070\n",
      "Epoch 230/300 - train_loss: 0.251739 - train_acc: 89.909088\n",
      "Epoch 231/300 - train_loss: 0.262202 - train_acc: 89.679420\n",
      "Epoch 232/300 - train_loss: 0.260157 - train_acc: 89.440186\n",
      "Epoch 233/300 - train_loss: 0.253365 - train_acc: 89.751190\n",
      "Epoch 234/300 - train_loss: 0.257973 - train_acc: 89.617218\n",
      "Epoch 235/300 - train_loss: 0.272530 - train_acc: 88.976074\n",
      "Epoch 236/300 - train_loss: 0.309867 - train_acc: 88.956932\n",
      "Epoch 237/300 - train_loss: 0.281301 - train_acc: 88.875595\n",
      "Epoch 238/300 - train_loss: 0.259095 - train_acc: 89.626793\n",
      "Epoch 239/300 - train_loss: 0.254898 - train_acc: 89.703346\n",
      "Epoch 240/300 - train_loss: 0.246258 - train_acc: 90.033493\n",
      "Epoch 241/300 - train_loss: 0.253633 - train_acc: 89.674637\n",
      "Epoch 242/300 - train_loss: 0.241964 - train_acc: 90.291862\n",
      "Epoch 243/300 - train_loss: 0.255387 - train_acc: 89.909088\n",
      "Epoch 244/300 - train_loss: 0.249371 - train_acc: 89.918655\n",
      "Epoch 245/300 - train_loss: 0.260984 - train_acc: 89.660286\n",
      "Epoch 246/300 - train_loss: 0.256037 - train_acc: 89.818176\n",
      "Epoch 247/300 - train_loss: 0.261604 - train_acc: 89.425835\n",
      "Epoch 248/300 - train_loss: 0.257219 - train_acc: 89.674637\n",
      "Epoch 249/300 - train_loss: 0.240064 - train_acc: 90.196167\n",
      "Epoch 250/300 - train_loss: 0.240180 - train_acc: 90.344498\n",
      "Epoch 251/300 - train_loss: 0.250095 - train_acc: 89.794258\n",
      "Epoch 252/300 - train_loss: 0.248297 - train_acc: 89.827751\n",
      "Epoch 253/300 - train_loss: 0.255848 - train_acc: 89.751190\n",
      "Epoch 254/300 - train_loss: 0.250330 - train_acc: 89.770332\n",
      "Epoch 255/300 - train_loss: 0.250344 - train_acc: 89.866028\n",
      "Epoch 256/300 - train_loss: 0.252718 - train_acc: 90.009567\n",
      "Epoch 257/300 - train_loss: 0.270398 - train_acc: 89.272720\n",
      "Epoch 258/300 - train_loss: 0.274911 - train_acc: 89.167458\n",
      "Epoch 259/300 - train_loss: 0.294863 - train_acc: 88.827751\n",
      "Epoch 260/300 - train_loss: 0.255207 - train_acc: 89.688995\n",
      "Epoch 261/300 - train_loss: 0.259024 - train_acc: 89.631577\n",
      "Epoch 262/300 - train_loss: 0.248150 - train_acc: 90.047844\n",
      "Epoch 263/300 - train_loss: 0.268912 - train_acc: 89.406693\n",
      "Epoch 264/300 - train_loss: 0.242669 - train_acc: 90.325356\n",
      "Epoch 265/300 - train_loss: 0.245455 - train_acc: 90.143539\n",
      "Epoch 266/300 - train_loss: 0.248062 - train_acc: 90.133965\n",
      "Epoch 267/300 - train_loss: 0.245797 - train_acc: 90.263153\n",
      "Epoch 268/300 - train_loss: 0.240321 - train_acc: 90.382774\n",
      "Epoch 269/300 - train_loss: 0.257683 - train_acc: 89.770332\n",
      "Epoch 270/300 - train_loss: 0.253919 - train_acc: 89.794258\n",
      "Epoch 271/300 - train_loss: 0.276570 - train_acc: 89.110046\n",
      "Epoch 272/300 - train_loss: 0.267809 - train_acc: 89.655502\n",
      "Epoch 273/300 - train_loss: 0.259409 - train_acc: 89.736839\n",
      "Epoch 274/300 - train_loss: 0.332136 - train_acc: 87.866028\n",
      "Epoch 275/300 - train_loss: 0.262790 - train_acc: 89.368416\n",
      "Epoch 276/300 - train_loss: 0.246875 - train_acc: 90.081337\n",
      "Epoch 277/300 - train_loss: 0.241070 - train_acc: 90.301430\n",
      "Epoch 278/300 - train_loss: 0.240925 - train_acc: 90.320572\n",
      "Epoch 279/300 - train_loss: 0.240187 - train_acc: 90.377991\n",
      "Epoch 280/300 - train_loss: 0.239139 - train_acc: 90.440186\n",
      "Epoch 281/300 - train_loss: 0.246593 - train_acc: 90.210526\n",
      "Epoch 282/300 - train_loss: 0.251703 - train_acc: 90.119614\n",
      "Epoch 283/300 - train_loss: 0.242968 - train_acc: 90.267937\n",
      "Epoch 284/300 - train_loss: 0.238837 - train_acc: 90.416267\n",
      "Epoch 285/300 - train_loss: 0.251104 - train_acc: 90.248802\n",
      "Epoch 286/300 - train_loss: 0.249359 - train_acc: 90.110046\n",
      "Epoch 287/300 - train_loss: 0.248758 - train_acc: 90.004784\n",
      "Epoch 288/300 - train_loss: 0.259710 - train_acc: 89.808609\n",
      "Epoch 289/300 - train_loss: 0.261056 - train_acc: 89.641144\n",
      "Epoch 290/300 - train_loss: 0.245998 - train_acc: 90.100471\n",
      "Epoch 291/300 - train_loss: 0.272559 - train_acc: 89.334923\n",
      "Epoch 292/300 - train_loss: 0.256123 - train_acc: 89.995209\n",
      "Epoch 293/300 - train_loss: 0.258107 - train_acc: 89.789467\n",
      "Epoch 294/300 - train_loss: 0.247694 - train_acc: 90.052628\n",
      "Epoch 295/300 - train_loss: 0.241947 - train_acc: 90.421051\n",
      "Epoch 296/300 - train_loss: 0.242517 - train_acc: 90.464111\n",
      "Epoch 297/300 - train_loss: 0.245041 - train_acc: 90.244019\n",
      "Epoch 298/300 - train_loss: 0.248683 - train_acc: 89.933014\n",
      "Epoch 299/300 - train_loss: 0.238974 - train_acc: 90.397125\n",
      "Epoch 300/300 - train_loss: 0.376530 - train_acc: 87.234444\n"
     ]
    }
   ],
   "source": [
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8730430625519663, 83.94339752197266)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    trainer.test()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "gcn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "11119ea4b95b49ccb656a7a65e90673b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_781c3f8aae4844b6942e9198e9706f5b",
       "IPY_MODEL_1b2087ddaab14acab394bb565434203e"
      ],
      "layout": "IPY_MODEL_bcb19d65b8c4409392ad357b90149e5e"
     }
    },
    "1b2087ddaab14acab394bb565434203e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1efa363c64184e988cd92b9ff0b879f1",
      "placeholder": "​",
      "style": "IPY_MODEL_1f5c969dbd8f41719a5850f8bc7c7406",
      "value": " 1224/1224 [00:12&lt;00:00, 96.34it/s]"
     }
    },
    "1efa363c64184e988cd92b9ff0b879f1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f5c969dbd8f41719a5850f8bc7c7406": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "781c3f8aae4844b6942e9198e9706f5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Computing transition probabilities: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9833efb2f49448768d5797689ff17cd2",
      "max": 1224,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a9189f6de19343ec9904ae92b067e136",
      "value": 1224
     }
    },
    "9833efb2f49448768d5797689ff17cd2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9189f6de19343ec9904ae92b067e136": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bcb19d65b8c4409392ad357b90149e5e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
