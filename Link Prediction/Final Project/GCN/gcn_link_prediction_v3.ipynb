{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rt4c6QLyCDC"
   },
   "source": [
    "## Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4ZxYZNY2yAJV"
   },
   "outputs": [],
   "source": [
    "# Base\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Graph\n",
    "import networkx as nx\n",
    "# node embedding\n",
    "from node2vec import Node2Vec\n",
    "# sklearn measure\n",
    "from sklearn import metrics\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9p5HjCc05FxF"
   },
   "outputs": [],
   "source": [
    "class GraphStructure():   \n",
    "    def __init__(self, G):\n",
    "          self.G = G\n",
    "\n",
    "    '''calucate disconnected pairs for negative sample'''\n",
    "    def disconnected_node_pairs(self, node_list):\n",
    "        possible_node_pairs = list()\n",
    "        adjacency_matrix = nx.to_numpy_array(self.G, nodelist=node_list)\n",
    "        for i in range(adjacency_matrix.shape[0]):\n",
    "            for j in range(adjacency_matrix.shape[1]):\n",
    "                if i != j:\n",
    "                    try:\n",
    "                        n = nx.shortest_path_length(G, str(i), str(j))\n",
    "                    except:\n",
    "                        n = 0\n",
    "                    if n <= 2 and adjacency_matrix[i, j] == 0:\n",
    "                        possible_node_pairs.append((node_list[i], node_list[j]))\n",
    "#                 if i != j and adjacency_matrix[i][j] == 0:\n",
    "#                     possible_node_pairs.append((node_list[i], node_list[j]))\n",
    "        return possible_node_pairs\n",
    "\n",
    "    '''calucate removable pairs for positive sample'''\n",
    "    def removable_node_pairs(self, node_pairs_df):\n",
    "        # check whether removing a node pair will cause\n",
    "        # 1: graphic segmentation\n",
    "        # 2: reduce the number of nodes\n",
    "        removable_links_index = list()\n",
    "        original_node_num = self.G.number_of_nodes()\n",
    "        temp_node_pairs_df = node_pairs_df.copy()\n",
    "        for i in tqdm(node_pairs_df.index.values):\n",
    "            temp_G = nx.from_pandas_edgelist(temp_node_pairs_df.drop(index = i), \"node1\", \"node2\", create_using=nx.Graph())\n",
    "            if (nx.number_connected_components(temp_G) == 1) and (temp_G.number_of_nodes() == original_node_num):\n",
    "                removable_links_index.append(i)\n",
    "                temp_node_pairs_df = temp_node_pairs_df.drop(index = i) \n",
    "        return removable_links_index\n",
    "\n",
    "def load_dataset(file_path, split_symbol, read_title=False):\n",
    "    node_pairs = list()\n",
    "    with open(file_path, 'r') as f:\n",
    "        if read_title:\n",
    "            title = f.readline()\n",
    "        for line in f.readlines():\n",
    "            node_pairs.append(list(line.strip().split(split_symbol)))\n",
    "        dataset_df = pd.DataFrame(node_pairs, columns=['node1', 'node2'])\n",
    "    return dataset_df\n",
    "\n",
    "def preprocess(node_pairs_df):\n",
    "    instances = list()\n",
    "    for i, row in node_pairs_df.iterrows():\n",
    "        s_index, t_index, label = row\n",
    "        instance = {\n",
    "            'source': torch.LongTensor(np.array([int(s_index)-1])),\n",
    "            'target': torch.LongTensor(np.array([int(t_index)-1])),\n",
    "            'label': torch.FloatTensor(np.array([float(label)]))\n",
    "        }\n",
    "        instances.append(instance)\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzsk_iJRnJMK"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-5RAaK_30bWj"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Random seed\n",
    "    seed = 42\n",
    "    train_sample_ratio = 0.8\n",
    "    valid_sample_ratio = 0.1\n",
    "    test_sample_ratio = 0.2\n",
    "    sample_rate = 1\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    node_pairs_df = load_dataset('out.dimacs10-polblogs', split_symbol='\\t', read_title=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnO26buXBQaQ"
   },
   "source": [
    "## Dataset Splitting and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmBPi_4T5FxI",
    "outputId": "5b8abfc7-632f-4210-f2f3-7331b38c1d3c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total # of nodes: 1224\n",
      "total # of edges: 16715\n"
     ]
    }
   ],
   "source": [
    "    # node_pairs = [ pair for pair in zip(node_pairs_df['node1'], node_pairs_df['node2'])]\n",
    "    last_snapshot = nx.from_pandas_edgelist(node_pairs_df, 'node1', 'node2', create_using=nx.Graph())\n",
    "    last_node_pairs_df = pd.DataFrame(list(last_snapshot.edges()), columns=['node1', 'node2']) \n",
    "    print('total # of nodes:', last_snapshot.number_of_nodes())\n",
    "    print('total # of edges:', last_snapshot.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pu-Rwyp45FxJ"
   },
   "source": [
    "#### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1J4VAENIKlyJ",
    "outputId": "7842c941-e28e-4e41-dd36-4a7703562c76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test # of negative: 1463522\t# of positive: 3343\n",
      "sample after:\n",
      "# of negative: 3343\t# of positive: 3343\n",
      "\n",
      "       node1 node2  label\n",
      "926176   483    66      0\n",
      "705040   841   898      0\n",
      "67873     61   398      0\n",
      "204386   181  1059      0\n",
      "385838   369   721      0\n",
      "...      ...   ...    ...\n",
      "16710   1091  1161      1\n",
      "16711   1117  1157      1\n",
      "16712   1168  1210      1\n",
      "16713   1180  1181      1\n",
      "16714   1189  1213      1\n",
      "\n",
      "[6686 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "    # Top 20% edges for test positive sample\n",
    "    test_positive_num = int(last_snapshot.number_of_edges()*test_sample_ratio)\n",
    "    test_positive_df = last_node_pairs_df.tail(test_positive_num).copy()\n",
    "    \n",
    "    # calculate unlink node pairs for test negative sample\n",
    "    test_gs = GraphStructure(last_snapshot)\n",
    "    test_no_edge_pairs = test_gs.disconnected_node_pairs(list(dict.fromkeys(last_node_pairs_df['node1'].to_list()+last_node_pairs_df['node2'].to_list())))\n",
    "    test_no_edge_pairs_df = pd.DataFrame(test_no_edge_pairs, columns=['node1', 'node2'])\n",
    "    test_negative_df = test_no_edge_pairs_df\n",
    "    \n",
    "    # labeling\n",
    "    test_negative_df['label'] = 0\n",
    "    test_positive_df['label'] = 1\n",
    "    print(\"test # of negative: %d\\t# of positive: %d\" % (len(test_negative_df), len(test_positive_df)))\n",
    "    \n",
    "    test_negative_df = test_negative_df.sample(int(len(test_positive_df)*sample_rate), replace=True)\n",
    "    test_dataset_df = test_negative_df.append(test_positive_df)\n",
    "    test_negative_num, test_positive_num = test_dataset_df.label.value_counts()\n",
    "    print(\"sample after:\\n# of negative: %d\\t# of positive: %d\\n\" % (test_positive_num, test_negative_num))\n",
    "    print(test_dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5WFMJzpf83Z",
    "outputId": "52e4fb73-d5ea-4e20-9385-698c8d4c8c00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of negative: 302572\t# of positive: 1337\n",
      "sample after:\n",
      "# of negative: 1337\t# of positive: 1337\n",
      "\n",
      "       node1 node2  label\n",
      "290170   772   733      0\n",
      "231397   957  1025      0\n",
      "138917   623   593      0\n",
      "173290  1022   447      0\n",
      "235966  1121   394      0\n",
      "...      ...   ...    ...\n",
      "13367    530   959      1\n",
      "13368    530   689      1\n",
      "13369    530   748      1\n",
      "13370    530   785      1\n",
      "13371    530   786      1\n",
      "\n",
      "[2674 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "    # temp\n",
    "    temp_positive_num = last_snapshot.number_of_edges() - test_positive_num\n",
    "    temp_positive_df = last_node_pairs_df.head(temp_positive_num).copy()\n",
    "    \n",
    "    \n",
    "    # Top 10% edges for train positive edge\n",
    "    valid_positive_num = int(temp_positive_num*valid_sample_ratio)\n",
    "    valid_positive_df = temp_positive_df.tail(valid_positive_num).copy()\n",
    "    valid_snapshot = nx.from_pandas_edgelist(valid_positive_df, 'node1', 'node2', create_using=nx.Graph())\n",
    "    \n",
    "    # remove edges with test positive sample for training snapshot\n",
    "    valid_snapshot = last_snapshot.copy()\n",
    "    for pair in zip(test_positive_df['node1'], test_positive_df['node2']):\n",
    "        valid_snapshot.remove_edge(*pair)\n",
    "    \n",
    "    # calculate unlink node pairs for train negative sample\n",
    "    valid_gs = GraphStructure(valid_snapshot)\n",
    "    valid_no_edge_pairs = valid_gs.disconnected_node_pairs(list(dict.fromkeys(valid_positive_df['node1'].to_list()+valid_positive_df['node2'].to_list())))\n",
    "    valid_no_edge_pairs_df = pd.DataFrame(valid_no_edge_pairs, columns=['node1', 'node2'])\n",
    "    valid_negative_df = valid_no_edge_pairs_df\n",
    "    \n",
    "    # labeling\n",
    "    valid_negative_df['label'] = 0\n",
    "    valid_positive_df['label'] = 1\n",
    "    print(\"# of negative: %d\\t# of positive: %d\" % (len(valid_negative_df), len(valid_positive_df)))\n",
    "\n",
    "    valid_negative_df = valid_negative_df.sample(len(valid_positive_df), replace=True)\n",
    "    valid_dataset_df = valid_negative_df.append(valid_positive_df)\n",
    "    valid_positive_num, valid_negative_num = valid_dataset_df.label.value_counts()\n",
    "    print(\"sample after:\\n# of negative: %d\\t# of positive: %d\\n\" % (valid_positive_num, valid_negative_num))\n",
    "    print(valid_dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOOEWqlG5FxL"
   },
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLET0xn8R_qa",
    "outputId": "612bf868-3819-450e-c99d-34c4558b2cdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of negative: 1213586\t# of positive: 12035\n",
      "sample after:\n",
      "# of negative: 12035\t# of positive: 12035\n",
      "\n",
      "       node1 node2  label\n",
      "954061  1127    83      0\n",
      "358119   375  1171      0\n",
      "8792       9   658      0\n",
      "439525    90   139      0\n",
      "949102   730   559      0\n",
      "...      ...   ...    ...\n",
      "12030    342   377      1\n",
      "12031    342   384      1\n",
      "12032    342   400      1\n",
      "12033    342   502      1\n",
      "12034    342   529      1\n",
      "\n",
      "[24070 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "    # Top 70% edges for train positive edge\n",
    "    train_positive_num = temp_positive_num - valid_positive_num\n",
    "    train_positive_df = temp_positive_df.head(train_positive_num).copy()\n",
    "    train_snapshot = nx.from_pandas_edgelist(train_positive_df, 'node1', 'node2', create_using=nx.Graph())\n",
    "    \n",
    "    # remove edges with test positive sample for training snapshot\n",
    "    train_snapshot = valid_snapshot.copy()\n",
    "    for pair in zip(valid_positive_df['node1'], valid_positive_df['node2']):\n",
    "        train_snapshot.remove_edge(*pair)\n",
    "    \n",
    "    # calculate unlink node pairs for train negative sample\n",
    "    train_gs = GraphStructure(train_snapshot)\n",
    "    train_no_edge_pairs = train_gs.disconnected_node_pairs(list(dict.fromkeys(train_positive_df['node1'].to_list()+train_positive_df['node2'].to_list())))\n",
    "    train_no_edge_pairs_df = pd.DataFrame(train_no_edge_pairs, columns=['node1', 'node2'])\n",
    "    train_negative_df = train_no_edge_pairs_df\n",
    "    \n",
    "    # labeling\n",
    "    train_negative_df['label'] = 0\n",
    "    train_positive_df['label'] = 1\n",
    "    print(\"# of negative: %d\\t# of positive: %d\" % (len(train_negative_df), len(train_positive_df)))\n",
    "\n",
    "    train_negative_df = train_negative_df.sample(len(train_positive_df), replace=True)\n",
    "    train_dataset_df = train_negative_df.append(train_positive_df)\n",
    "    train_positive_num, train_negative_num = train_dataset_df.label.value_counts()\n",
    "    print(\"sample after:\\n# of negative: %d\\t# of positive: %d\\n\" % (train_positive_num, train_negative_num))\n",
    "    print(train_dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T890E3P55FxM"
   },
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geVaSGGd5FxM",
    "outputId": "254e62c6-7af6-4dd2-bb3a-7818d7b2ddc3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of test instances: 6686\n",
      "# of valid instances: 2674\n",
      "# of train instances: 24070\n",
      "# of total instances: 33430\n"
     ]
    }
   ],
   "source": [
    "    test_instances = preprocess(test_dataset_df)\n",
    "    valid_instances = preprocess(valid_dataset_df)\n",
    "    train_instances = preprocess(train_dataset_df)\n",
    "    \n",
    "    print('# of test instances:', len(test_instances))\n",
    "    print('# of valid instances:', len(valid_instances))\n",
    "    print('# of train instances:', len(train_instances))\n",
    "    print('# of total instances:', (len(train_instances)+len(valid_instances)+len(test_instances)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rTSUoWMOmeZ"
   },
   "source": [
    "## Graph Node Embedding with Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "f152c8a5153c4235bd37d9d47cf48830",
      "b7db93717d7643e4bf0777503d87cab2",
      "de30f42132da46f082cee8dfcf051c2f",
      "0e7ae792d1264c1db57e178901615630",
      "019bfd6e6d68453ab8302e32ff4c693e",
      "a2ec94874bad43b0a297e6e368641ce8",
      "52dda8a6eec84634acb241491d4b8e71",
      "8b3f14adca0e4d919011a7cae002d3fd"
     ]
    },
    "id": "K-91l9ElOnDw",
    "outputId": "0cd5aabe-f6b4-491e-e1d4-1916bfbcf847",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8b778327f745ba9582b3b60e56688c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Computing transition probabilities'), FloatProgress(value=0.0, max=1224.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1):   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [00:35<00:00,  3.53s/it]\n"
     ]
    }
   ],
   "source": [
    "    node2vec = Node2Vec(train_snapshot, dimensions=128, walk_length=80, num_walks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bqsoVI4PtZXY"
   },
   "outputs": [],
   "source": [
    "    n2v_model = node2vec.fit(window=10, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1A7aBo65FxN",
    "outputId": "7e122291-2177-4c35-9796-d90e8f6e95e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1224, 128)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    node_embedding = n2v_model.wv.vectors\n",
    "    node_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frU6f-Z45FxO"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Dm4wkWxc5FxO"
   },
   "outputs": [],
   "source": [
    "    class NodePairDataset(Dataset):\n",
    "        def __init__(self, instances):\n",
    "            self.instances = instances\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.instances)\n",
    "\n",
    "        def __getitem__(self, i):\n",
    "            instance = self.instances[i]\n",
    "            source = instance['source']\n",
    "            target = instance['target']\n",
    "            label = instance['label']\n",
    "            return source, target, label\n",
    "        \n",
    "    def collate_fn(batch):\n",
    "        source, target, labels = zip(*batch)\n",
    "        source = torch.stack(source)\n",
    "        target = torch.stack(target)\n",
    "        labels = torch.stack(labels)\n",
    "        return source, target, labels\n",
    "\n",
    "    def get_dataloader(instances, collate_fn=collate_fn,batch_size=1, num_workers=2):\n",
    "        dataset = NodePairDataset(instances)\n",
    "        dataloader = DataLoader(dataset, collate_fn=collate_fn, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bg6RaO_85FxO"
   },
   "outputs": [],
   "source": [
    "    class LinkEmbedding(nn.Module):\n",
    "        def __init__(self, inputs_dim, output_dim):\n",
    "            super(LinkEmbedding, self).__init__()\n",
    "            self.weight = nn.Parameter(nn.init.xavier_uniform_(torch.empty(inputs_dim, output_dim)))\n",
    "            \n",
    "            \n",
    "        def forward(self, hidden_state, source, target):\n",
    "            propagation = torch.mul(hidden_state[source, :], hidden_state[target, :])\n",
    "            propagation = propagation.matmul(self.weight)\n",
    "            return propagation\n",
    "    \n",
    "    class GraphConvolution(nn.Module):\n",
    "        def __init__(self, inputs_dim, hidden_dim):\n",
    "            super(GraphConvolution, self).__init__()\n",
    "            self.weight = nn.Parameter(nn.init.kaiming_normal_(torch.empty(inputs_dim, hidden_dim), mode='fan_in', nonlinearity='relu'))\n",
    "            # self.weight = nn.Parameter(nn.init.xavier_uniform_(torch.empty(inputs_dim, hidden_dim)))\n",
    "            \n",
    "            \n",
    "        def forward(self, input_features, adj_matrix):\n",
    "            # aggregate \n",
    "            aggregate  = torch.mm(input_features, self.weight)\n",
    "            propagation = torch.mm(adj_matrix, aggregate)\n",
    "            return propagation\n",
    "        \n",
    "    class GCN(nn.Module):\n",
    "        def __init__(self, inputs_dim, hidden_dim, output_dim):\n",
    "            super(GCN, self).__init__()\n",
    "            self.gcn_layer1 = GraphConvolution(inputs_dim, hidden_dim)\n",
    "            self.gcn_layer2 = GraphConvolution(hidden_dim, hidden_dim)\n",
    "            self.link_embed_layer = LinkEmbedding(hidden_dim, output_dim)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, input_features, adj_matrix, source, target):\n",
    "            hidden_state = self.relu(self.gcn_layer1(input_features, adj_matrix))\n",
    "            hidden_state = self.gcn_layer2(hidden_state, adj_matrix)\n",
    "            hidden_state = self.link_embed_layer(hidden_state, source, target)\n",
    "            return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "2IAyLfi25FxO"
   },
   "outputs": [],
   "source": [
    "    class GCNTrainer():\n",
    "        def __init__(self, features, adj_matrix, train_instances, valid_instances=None, test_instances=None, \n",
    "            hidden_dim=16, epoch=1, max_patience=0, learning_rate=1e-2, batch_size=1,num_workers=2, valid=False):\n",
    "\n",
    "            # parameters\n",
    "            self.valid = valid\n",
    "            self.epochs = epoch\n",
    "            self.learning_rate = learning_rate\n",
    "            self.batch_size = batch_size\n",
    "            self.num_workers = num_workers\n",
    "            # early stop\n",
    "            self.best_valid_loss = 1e10\n",
    "            self.max_patience = max_patience\n",
    "            self.patience = 0\n",
    "\n",
    "            # setup cuda device\n",
    "            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            # dataset\n",
    "            self.train_instances = train_instances\n",
    "            self.valid_instances = valid_instances\n",
    "            self.test_instances = test_instances\n",
    "            self.features = torch.FloatTensor(features).cuda()\n",
    "            self.adj_matrix = torch.FloatTensor(self.normalize(adj_matrix)).cuda()\n",
    "            \n",
    "            # GCN Model\n",
    "            self.model = GCN(self.features.shape[1], hidden_dim, output_dim=1)\n",
    "            self.model.cuda()\n",
    "            # print(self.model)\n",
    "\n",
    "            # Adam optimizer with hyper-parameter\n",
    "            # self.optimizer = SGD(self.model.parameters(), lr=self.learning_rate)\n",
    "            self.optimizer = Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "            # Binary Cross Entropy with Loss for criterion\n",
    "            self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "\n",
    "        def normalize(self, A):\n",
    "            '''\n",
    "            :var I: identity matrix\n",
    "            :var A: adjacency matrix\n",
    "            :var D: degree matrix\n",
    "            :var A_hat: adding self-loops\n",
    "            :var D_inv: degree inverse matrix\n",
    "            '''\n",
    "            I = np.matrix(np.identity(A.shape[0]))\n",
    "            A_hat = I + A\n",
    "            \n",
    "            D = np.array(np.sum(A, axis=0))\n",
    "            D_inv = D**-0.5\n",
    "            D_inv[np.isinf(D_inv)] = 0.\n",
    "            D_inv = np.diag(D_inv)\n",
    "\n",
    "            A_hat = D_inv * A_hat * D_inv\n",
    "            return A_hat\n",
    "        \n",
    "        def accuracy(self, predicts, labels):\n",
    "            predicts_labels = torch.round(torch.sigmoid(predicts))\n",
    "            total_correct = (predicts_labels == labels).sum().float()\n",
    "            return torch.round((total_correct / labels.shape[0]) * 100)\n",
    "\n",
    "        def train(self):\n",
    "            start_time = time()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "                train_dataloader = get_dataloader(self.train_instances, collate_fn=collate_fn, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "                self.model.train()\n",
    "                epoch_loss, epoch_acc = 0, 0\n",
    "                ''' train '''\n",
    "                for i, batch in enumerate(train_dataloader, start=1):\n",
    "                    batch = (tensor.cuda() for tensor in batch)\n",
    "                    source, target, labels = batch\n",
    "                    # forward\n",
    "                    # feature: all node embedding\n",
    "                    outputs = self.model(self.features, self.adj_matrix, source, target)\n",
    "                    outputs = outputs.reshape(labels.size())\n",
    "                    # backward\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    acc = self.accuracy(outputs, labels)\n",
    "                    epoch_loss += loss.item()\n",
    "                    epoch_acc += acc\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    # optimize\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    # Progressbar\n",
    "                    elapsed_time = time() - start_time\n",
    "                    elapsed_time = timedelta(seconds=int(elapsed_time))\n",
    "                    # print(\"Epoch %d/%d | loss: %.6f | acc: %f | batch: [%d/%d] | %s\" % (epoch+1, self.epochs, loss, acc, i, len(train_dataloader), elapsed_time))\n",
    "                \n",
    "                print(\"Epoch %d/%d - train_loss: %.6f - train_acc: %.2f%%\" \n",
    "                      % (epoch+1, self.epochs, epoch_loss/len(train_dataloader), epoch_acc/len(train_dataloader)))\n",
    "                \n",
    "                ''' validate '''\n",
    "                if self.valid:\n",
    "                    valid_loss, valid_acc = self.validate()\n",
    "                    elapsed_time = time() - start_time\n",
    "                    elapsed_time = timedelta(seconds=int(elapsed_time))\n",
    "                    print(\"Epoch %d/%d - valid_loss: %.6f - valid_acc: %.2f%%\" % (epoch+1, self.epochs, valid_loss, valid_acc))\n",
    "\n",
    "                    # early stoping\n",
    "                    if valid_loss < self.best_valid_loss:\n",
    "                        self.patience = 0\n",
    "                        self.best_valid_loss = valid_loss\n",
    "                    else:\n",
    "                        self.patience += 1\n",
    "\n",
    "                    if self.patience > self.max_patience:\n",
    "                        print('Earlystop at epoch %d' % (epoch+1))\n",
    "                        break\n",
    "\n",
    "\n",
    "        def validate(self):\n",
    "            total_loss, total_acc = 0, 0\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                valid_dataloader = get_dataloader(self.valid_instances, collate_fn=collate_fn, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "                for batch in valid_dataloader:\n",
    "                    batch = (tensor.cuda() for tensor in batch)\n",
    "                    source, target, labels = batch\n",
    "                    outputs = self.model(self.features, self.adj_matrix, source, target)\n",
    "                    outputs = outputs.reshape(labels.size())\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    # loss and accuracy\n",
    "                    total_loss += loss.item()\n",
    "                    total_acc += self.accuracy(outputs, labels)\n",
    "            \n",
    "            total_loss /= len(valid_dataloader)\n",
    "            total_acc /= len(valid_dataloader)\n",
    "            return float(total_loss), float(total_acc)\n",
    "\n",
    "        def test(self):\n",
    "            total_loss, total_acc, auc = 0, 0, 0\n",
    "            total_predicts, total_labels = list(), list()\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_dataloader = get_dataloader(self.test_instances, collate_fn=collate_fn, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "                for batch in test_dataloader:\n",
    "                    batch = (tensor.cuda() for tensor in batch)\n",
    "                    source, target, labels = batch\n",
    "                    outputs = self.model(self.features, self.adj_matrix, source, target)\n",
    "                    outputs = outputs.reshape(labels.size())\n",
    "                    # auc\n",
    "                    total_predicts += torch.round(torch.sigmoid(outputs.cpu())).squeeze().numpy().tolist()\n",
    "                    total_labels += torch.round(torch.sigmoid(labels.cpu())).squeeze().numpy().tolist()\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    # loss and accuracy\n",
    "                    total_loss += loss.item()\n",
    "                    total_acc += self.accuracy(outputs, labels)\n",
    "            \n",
    "            total_loss /= len(test_dataloader)\n",
    "            total_acc /= len(test_dataloader)\n",
    "            \n",
    "            fpr, tpr, thresholds = metrics.roc_curve(total_labels, total_predicts, pos_label=1)\n",
    "            auc = metrics.auc(fpr, tpr)\n",
    "            return float(total_loss), float(total_acc), auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTS4-KQH5FxP",
    "outputId": "c4fc3b90-f503-4301-a524-09298c72c4e8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "    adj_matrix = nx.to_numpy_array(train_snapshot)\n",
    "    print(adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_FrH6kq5FxP",
    "outputId": "844b1e6b-2a5e-42a3-c554-c26135d2266f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-05aee304eea4>:51: RuntimeWarning: divide by zero encountered in power\n",
      "  D_inv = D**-0.5\n"
     ]
    }
   ],
   "source": [
    "    trainer = GCNTrainer(features=node_embedding, adj_matrix=adj_matrix, \n",
    "                         train_instances=train_instances, \n",
    "                         valid_instances=valid_instances,\n",
    "                         test_instances=test_instances,\n",
    "                         hidden_dim=64, epoch=300, max_patience=5, learning_rate=1e-2, batch_size=128, num_workers=2, valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4thQlWPX38zg",
    "outputId": "e0c37871-cbe3-44cc-cabc-24b47d3f4c7e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 - train_loss: 0.529236 - train_acc: 69.68%\n",
      "Epoch 1/300 - valid_loss: 0.738636 - valid_acc: 47.62%\n",
      "Epoch 2/300 - train_loss: 0.397948 - train_acc: 83.68%\n",
      "Epoch 2/300 - valid_loss: 1.540079 - valid_acc: 50.43%\n",
      "Epoch 3/300 - train_loss: 0.346454 - train_acc: 86.49%\n",
      "Epoch 3/300 - valid_loss: 2.673313 - valid_acc: 50.57%\n",
      "Epoch 4/300 - train_loss: 0.329917 - train_acc: 86.95%\n",
      "Epoch 4/300 - valid_loss: 1.850046 - valid_acc: 50.33%\n",
      "Epoch 5/300 - train_loss: 0.321151 - train_acc: 87.58%\n",
      "Epoch 5/300 - valid_loss: 2.190752 - valid_acc: 49.90%\n",
      "Epoch 6/300 - train_loss: 0.315631 - train_acc: 87.49%\n",
      "Epoch 6/300 - valid_loss: 3.096268 - valid_acc: 50.38%\n",
      "Epoch 7/300 - train_loss: 0.300973 - train_acc: 88.30%\n",
      "Epoch 7/300 - valid_loss: 4.491056 - valid_acc: 50.10%\n",
      "Earlystop at epoch 7\n"
     ]
    }
   ],
   "source": [
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HcdROfn39H1m",
    "outputId": "5719ee5b-c3a8-4955-e1d9-7b7c6fb30c91",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss:3.986682 | test_acc:48.96 | auc:0.49\n"
     ]
    }
   ],
   "source": [
    "    loss, accuracy, auc = trainer.test()\n",
    "    print('test_loss:%.6f | test_acc:%.2f | auc:%.2f' % (loss, accuracy, auc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gcn_link_prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "019bfd6e6d68453ab8302e32ff4c693e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0e7ae792d1264c1db57e178901615630": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b3f14adca0e4d919011a7cae002d3fd",
      "placeholder": "​",
      "style": "IPY_MODEL_52dda8a6eec84634acb241491d4b8e71",
      "value": " 1224/1224 [00:06&lt;00:00, 195.28it/s]"
     }
    },
    "52dda8a6eec84634acb241491d4b8e71": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b3f14adca0e4d919011a7cae002d3fd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2ec94874bad43b0a297e6e368641ce8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7db93717d7643e4bf0777503d87cab2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de30f42132da46f082cee8dfcf051c2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Computing transition probabilities: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2ec94874bad43b0a297e6e368641ce8",
      "max": 1224,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_019bfd6e6d68453ab8302e32ff4c693e",
      "value": 1224
     }
    },
    "f152c8a5153c4235bd37d9d47cf48830": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_de30f42132da46f082cee8dfcf051c2f",
       "IPY_MODEL_0e7ae792d1264c1db57e178901615630"
      ],
      "layout": "IPY_MODEL_b7db93717d7643e4bf0777503d87cab2"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
